{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "HVKw9vAnO5cB",
    "outputId": "0688f391-cc4e-44a3-fd47-a34016ecf755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (1.41.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (4.25.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.17.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (1.12.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (3.17.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\public\\anc\\lib\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\public\\anc\\lib\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.23.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\public\\anc\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\public\\anc\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (1.41.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.17.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (1.12.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (4.25.2)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (3.17.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\public\\anc\\lib\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\public\\anc\\lib\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.23.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\public\\anc\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\public\\anc\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: google-auth in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (2.27.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\public\\anc\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-aiplatform\n",
    "!pip install google-auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Vertex AI library\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Import service account \n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(\"resumeanz-db0c67277823.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'resumeanz'\n",
    "location = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project='resumeanz',credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.TabularDataset.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4p3TVNzwO74s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (1.41.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (4.25.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.0.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (1.12.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.17.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (3.17.2)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.62.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\public\\anc\\lib\\site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\public\\anc\\lib\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.23.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\public\\anc\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pratik's predator\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\public\\anc\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\public\\anc\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\public\\anc\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCmmK0t0PG4j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "F4qNscTKPh27"
   },
   "outputs": [],
   "source": [
    "person_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. First, look for the Person Entity type in the text and extract the needed information defined below:\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. Document must be summarized and stored inside Person entity under `description` property\n",
    "    Entity Types:\n",
    "    label:'Person',id:string,role:string,description:string //Person Node\n",
    "2. Description property should be a crisp text summary and MUST NOT be more than 100 characters\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities\n",
    "5. Restrict yourself to extract only Person information. No Position, Company, Education or Skill information should be focussed.\n",
    "6. NEVER Impute missing values\n",
    "Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Person\",\"id\":\"person1\",\"role\":\"Prompt Developer\",\"description\":\"Prompt Developer with more than 30 years of LLM experience\"}]}\n",
    "\n",
    "Question: Now, extract the Person for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2lBKyQqAPlp_"
   },
   "outputs": [],
   "source": [
    "postion_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities & relationships strictly as instructed below\n",
    "1. First, look for Position & Company types in the text and extract information in comma-separated format. Position Entity denotes the Person's previous or current job. Company node is the Company where they held that position.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Position',id:string,title:string,location:string,startDate:string,endDate:string,url:string //Position Node\n",
    "    label:'Company',id:string,name:string //Company Node\n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. NEVER create new Relationship types that aren't mentioned below:\n",
    "    Relationship definition:\n",
    "    position|AT_COMPANY|company //Ensure this is a string in the generated output\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities.\n",
    "5. No Education or Skill information should be extracted.\n",
    "6. DO NOT MISS out any Position or Company related information\n",
    "7. NEVER Impute missing values\n",
    " Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Position\",\"id\":\"position1\",\"title\":\"Software Engineer\",\"location\":\"Singapore\",startDate:\"2021-01-01\",endDate:\"present\"},{\"label\":\"Position\",\"id\":\"position2\",\"title\":\"Senior Software Engineer\",\"location\":\"Mars\",startDate:\"2020-01-01\",endDate:\"2020-12-31\"},{label:\"Company\",id:\"company1\",name:\"Neo4j Singapore Pte Ltd\"},{\"label\":\"Company\",\"id\":\"company2\",\"name\":\"Neo4j Mars Inc\"}],\"relationships\": [\"position1|AT_COMPANY|company1\",\"position2|AT_COMPANY|company2\"]}\n",
    "\n",
    "Question: Now, extract entities & relationships as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_V-GwE64PqAG"
   },
   "outputs": [],
   "source": [
    "skill_prompt_tpl=\"\"\"From the Resume text below, extract Entities strictly as instructed below\n",
    "1. Look for prominent Skill Entities in the text. The`id` property of each entity must be alphanumeric and must be unique among the entities. NEVER create new entity types that aren't mentioned below:\n",
    "    Entity Definition:\n",
    "    label:'Skill',id:string,name:string,level:string //Skill Node\n",
    "2. NEVER Impute missing values\n",
    "3. If you do not find any level information: assume it as `expert` if the experience in that skill is more than 5 years, `intermediate` for 2-5 years and `beginner` otherwise.\n",
    "Example Output Format:\n",
    "{\"entities\": [{\"label\":\"Skill\",\"id\":\"skill1\",\"name\":\"Neo4j\",\"level\":\"expert\"},{\"label\":\"Skill\",\"id\":\"skill2\",\"name\":\"Pytorch\",\"level\":\"expert\"}]}\n",
    "\n",
    "Question: Now, extract entities as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1BTBwfWKPseL"
   },
   "outputs": [],
   "source": [
    "edu_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. Look for Education entity type and generate the information defined below:\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create other entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Definition:\n",
    "    label:'Education',id:string,degree:string,university:string,graduationDate:string,score:string,url:string //Education Node\n",
    "2. If you cannot find any information on the entities above, it is okay to return empty value. DO NOT create fictious data\n",
    "3. Do NOT create duplicate entities or properties\n",
    "4. Strictly extract only Education. No Skill or other Entities should be extracted\n",
    "5. DO NOT MISS out any Education related entity\n",
    "6. NEVER Impute missing values\n",
    "Output JSON (Strict):\n",
    "{\"entities\": [{\"label\":\"Education\",\"id\":\"education1\",\"degree\":\"Bachelor of Science\",\"graduationDate\":\"May 2022\",\"score\":\"0.0\"}]}\n",
    "\n",
    "Question: Now, extract Education information as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SU2QekzGPvfg"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "def run_text_model(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    location: str = location,\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    vertexai.init(project='resumeanz',credentials=credentials)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iIruatQzPybX"
   },
   "outputs": [],
   "source": [
    "def extract_entities_relationships(prompt, tuned_model_name):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ccrosqMvP3sC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ja8fO779P6uU"
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import json\n",
    "\n",
    "sample_que = \"\"\"Developer <span class=\"hl\">Developer</span> Developer - TATA CONSULTANTCY SERVICE Batavia, OH Relevant course work† Database Systems, Database Administration, Database Security & Auditing, Computer Security,Computer Networks, Programming & Software Development, IT, Information Security Concept & Admin,† IT System Acquisition & Integration, Advanced Web Development, and Ethical Hacking: Network Security & Pen Testing. Work Experience Developer TATA CONSULTANTCY SERVICE June 2016 to Present MRM (Government of ME, RI, MS) Developer†††† Working with various technologies such as Java, JSP, JSF, DB2(SQL), LDAP, BIRT report, Jazz version control, Squirrel SQL client, Hibernate, CSS, Linux, and Windows. Work as part of a team that provide support to enterprise applications. Perform miscellaneous support activities as requested by Management. Perform in-depth research and identify sources of production issues.†† SPLUNK Developer† Supporting the Splunk Operational environment for Business Solutions Unit aiming to support overall business infrastructure. Developing Splunk Queries to generate the report, monitoring, and analyzing machine generated big data for server that has been using for onsite and offshore team. Working with Splunk' premium apps such as ITSI, creating services, KPI, and glass tables. Developing app with custom dashboard with front- end ability and advanced XML to serve Business Solution unit' needs. Had in-house app presented at Splunk's .Conf Conference (2016). Help planning, prioritizing and executing development activities. Developer ( front end) intern TOMORROW PICTURES INC - Atlanta, GA April 2015 to January 2016 Assist web development team with multiple front end web technologies and involved in web technologies such as Node.js, express, json, gulp.js, jade, sass, html5, css3, bootstrap, WordPress.†Testing (manually), version control (GitHub), mock up design and ideas Education MASTER OF SCIENCE IN INFORMATION TECHNOLOGY in INFOTMATION TECHNOLOGY KENNESAW STATE UNIVERSITY - Kennesaw, GA August 2012 to May 2015 MASTER OF BUSINESS ADMINISTRATION in INTERNATIONAL BUSINESS AMERICAN INTER CONTINENTAL UNIVERSITY ATLANTA November 2003 to December 2005 BACHELOR OF ARTS in PUBLIC RELATIONS THE UNIVERSITY OF THAI CHAMBER OF COMMERCE - BANGKOK, TH June 1997 to May 2001 Skills Db2 (2 years), front end (2 years), Java (2 years), Linux (2 years), Splunk (2 years), SQL (3 years) Certifications/Licenses Splunk Certified Power User V6.3 August 2016 to Present CERT-112626 Splunk Certified Power User V6.x May 2017 to Present CERT-168138 Splunk Certified User V6.x May 2017 to Present CERT -181476 Driver's License Additional Information Skills† ∑††††SQL, PL/SQL, Knowledge of Data Modeling, Experience on Oracle database/RDBMS.† ∑††††††††Database experience on Oracle, DB2, SQL Sever, MongoDB, and MySQL.† ∑††††††††Knowledge of tools including Splunk, tableau, and wireshark.† ∑††††††††Knowledge of SCRUM/AGILE and WATERFALL methodologies.† ∑††††††††Web technology included: HTML5, CSS3, XML, JSON, JavaScript, node.js, NPM, GIT, express.js, jQuery, Angular, Bootstrap, and Restful API.† ∑††††††††Working Knowledge in JAVA, J2EE, and PHP.† Operating system Experience included: Windows, Mac OS, Linux (Ubuntu, Mint, Kali)††\"\"\"\n",
    "prompts = [person_prompt_tpl, postion_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "results = {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "for p in prompts:\n",
    "    _prompt = Template(p).substitute(ctext=clean_text(sample_que))\n",
    "    _extraction = extract_entities_relationships(_prompt, '')\n",
    "    if 'Answer:\\n' in _extraction:\n",
    "        _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "    if _extraction.strip() == '':\n",
    "        continue\n",
    "    try:\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\").replace('`', ''))\n",
    "    except json.JSONDecodeError:\n",
    "        # print(_extraction)\n",
    "        #Temp hack to ignore Skills cut off by token limitation\n",
    "        _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "    results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "    if \"relationships\" in _extraction:\n",
    "        results[\"relationships\"].extend(_extraction[\"relationships\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1K6PB9RJQIHi"
   },
   "outputs": [],
   "source": [
    "person_id = results[\"entities\"][0][\"id\"]\n",
    "for e in results[\"entities\"][1:]:\n",
    "    if e['label'] == 'Position':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "    if e['label'] == 'Skill':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "    if e['label'] == 'Education':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6CJUNZ0QTd2",
    "outputId": "b1999147-8c97-4509-dfc2-3cec5c4c7fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'label': 'Person',\n",
       "   'id': 'person1',\n",
       "   'role': 'Developer',\n",
       "   'description': 'Developer with 10+ years of experience in IT industry.'},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position1',\n",
       "   'title': 'Developer',\n",
       "   'location': 'Batavia, OH',\n",
       "   'startDate': '2016-06-01',\n",
       "   'endDate': 'present'},\n",
       "  {'label': 'Company', 'id': 'company1', 'name': 'TATA CONSULTANTCY SERVICE'},\n",
       "  {'label': 'Skill', 'id': 'skill1', 'name': 'SQL', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill2', 'name': 'Java', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill3', 'name': 'Linux', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill4', 'name': 'Splunk', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill5', 'name': 'front end', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill6', 'name': 'Db2', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill7', 'name': 'HTML5', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill8', 'name': 'CSS3', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill9', 'name': 'XML', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill10', 'name': 'JSON', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill11', 'name': 'JavaScript', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill12', 'name': 'node.js', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill13', 'name': 'NPM', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill14', 'name': 'GIT', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill15', 'name': 'express.js', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill16', 'name': 'jQuery', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill17', 'name': 'Angular', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill18', 'name': 'Bootstrap', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill19',\n",
       "   'name': 'Restful API',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill20', 'name': 'JAVA', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill21', 'name': 'J2EE', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill22', 'name': 'PHP', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill23', 'name': 'Windows', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill24', 'name': 'Mac OS', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill25', 'name': 'Linux', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill26', 'name': 'Ubuntu', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill27', 'name': 'Mint', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill28', 'name': 'Kali', 'level': 'expert'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education1',\n",
       "   'degree': 'Bachelor of Arts',\n",
       "   'university': 'THE UNIVERSITY OF THAI CHAMBER OF COMMERCE - BANGKOK, TH',\n",
       "   'graduationDate': 'May 2001',\n",
       "   'score': '0.0'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education2',\n",
       "   'degree': 'Master of Business Administration',\n",
       "   'university': 'AMERICAN INTER CONTINENTAL UNIVERSITY ATLANTA',\n",
       "   'graduationDate': 'December 2005',\n",
       "   'score': '0.0'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education3',\n",
       "   'degree': 'Master of Science in Information Technology',\n",
       "   'university': 'KENNESAW STATE UNIVERSITY - Kennesaw, GA',\n",
       "   'graduationDate': 'May 2015',\n",
       "   'score': '0.0'}],\n",
       " 'relationships': ['position1|AT_COMPANY|company1',\n",
       "  'person1|HAS_POSITION|position1',\n",
       "  'person1|HAS_SKILL|skill1',\n",
       "  'person1|HAS_SKILL|skill2',\n",
       "  'person1|HAS_SKILL|skill3',\n",
       "  'person1|HAS_SKILL|skill4',\n",
       "  'person1|HAS_SKILL|skill5',\n",
       "  'person1|HAS_SKILL|skill6',\n",
       "  'person1|HAS_SKILL|skill7',\n",
       "  'person1|HAS_SKILL|skill8',\n",
       "  'person1|HAS_SKILL|skill9',\n",
       "  'person1|HAS_SKILL|skill10',\n",
       "  'person1|HAS_SKILL|skill11',\n",
       "  'person1|HAS_SKILL|skill12',\n",
       "  'person1|HAS_SKILL|skill13',\n",
       "  'person1|HAS_SKILL|skill14',\n",
       "  'person1|HAS_SKILL|skill15',\n",
       "  'person1|HAS_SKILL|skill16',\n",
       "  'person1|HAS_SKILL|skill17',\n",
       "  'person1|HAS_SKILL|skill18',\n",
       "  'person1|HAS_SKILL|skill19',\n",
       "  'person1|HAS_SKILL|skill20',\n",
       "  'person1|HAS_SKILL|skill21',\n",
       "  'person1|HAS_SKILL|skill22',\n",
       "  'person1|HAS_SKILL|skill23',\n",
       "  'person1|HAS_SKILL|skill24',\n",
       "  'person1|HAS_SKILL|skill25',\n",
       "  'person1|HAS_SKILL|skill26',\n",
       "  'person1|HAS_SKILL|skill27',\n",
       "  'person1|HAS_SKILL|skill28',\n",
       "  'person1|HAS_EDUCATION|education1',\n",
       "  'person1|HAS_EDUCATION|education2',\n",
       "  'person1|HAS_EDUCATION|education3']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "af37jjsnRFjx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_prop_str(prop_dict, _id):\n",
    "    s = []\n",
    "    for key, val in prop_dict.items():\n",
    "      if key != 'label' and key != 'id':\n",
    "         s.append(_id+\".\"+key+' = \"'+str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"')\n",
    "    return ' ON CREATE SET ' + ','.join(s)\n",
    "\n",
    "def get_cypher_compliant_var(_id):\n",
    "    s = \"_\"+ re.sub(r'[\\W_]', '', _id).lower() #avoid numbers appearing as firstchar; replace spaces\n",
    "    return s[:20] #restrict variable size\n",
    "\n",
    "def generate_cypher(file_name, in_json):\n",
    "    e_map = {}\n",
    "    e_stmt = []\n",
    "    r_stmt = []\n",
    "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
    "    r_stmt_tpl = Template(\"\"\"\n",
    "      MATCH $src\n",
    "      MATCH $tgt\n",
    "      MERGE ($src_id)-[:$rel]->($tgt_id)\n",
    "    \"\"\")\n",
    "    for obj in in_json:\n",
    "      for j in obj['entities']:\n",
    "          props = ''\n",
    "          label = j['label']\n",
    "          id = ''\n",
    "          if label == 'Person':\n",
    "            id = 'p'+str(file_name)\n",
    "          elif label == 'Position':\n",
    "            c = j['id'].replace('position', '_')\n",
    "            id = f'j{str(file_name)}{c}'\n",
    "          elif label == 'Education':\n",
    "            c = j['id'].replace('education', '_')\n",
    "            id = f'e{str(file_name)}{c}'\n",
    "          else:\n",
    "            id = get_cypher_compliant_var(j['name'])\n",
    "          if label in ['Person', 'Position', 'Education', 'Skill', 'Company']:\n",
    "            varname = get_cypher_compliant_var(j['id'])\n",
    "            stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
    "            e_map[varname] = stmt\n",
    "            e_stmt.append('MERGE '+ stmt + get_prop_str(j, varname))\n",
    "\n",
    "      for st in obj['relationships']:\n",
    "          rels = st.split(\"|\")\n",
    "          src_id = get_cypher_compliant_var(rels[0].strip())\n",
    "          rel = rels[1].strip()\n",
    "          if rel in ['HAS_SKILL', 'HAS_EDUCATION', 'AT_COMPANY', 'HAS_POSITION']: #we ignore other relationships\n",
    "            tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
    "            stmt = r_stmt_tpl.substitute(\n",
    "              src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
    "            r_stmt.append(stmt)\n",
    "\n",
    "    return e_stmt, r_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTbyNxHcSLZb",
    "outputId": "631d5d89-c965-45b1-ff75-92dc24a0fa01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MERGE (_person1:Person{id:\\'pmy_cv\\'}) ON CREATE SET _person1.role = \"Developer\",_person1.description = \"Developer with 10+ years of experience in IT industry.\"', 'MERGE (_position1:Position{id:\\'jmy_cv_1\\'}) ON CREATE SET _position1.title = \"Developer\",_position1.location = \"Batavia, OH\",_position1.startDate = \"2016-06-01\",_position1.endDate = \"present\"', 'MERGE (_company1:Company{id:\\'_tataconsultantcyser\\'}) ON CREATE SET _company1.name = \"TATA CONSULTANTCY SERVICE\"', 'MERGE (_skill1:Skill{id:\\'_sql\\'}) ON CREATE SET _skill1.name = \"SQL\",_skill1.level = \"expert\"', 'MERGE (_skill2:Skill{id:\\'_java\\'}) ON CREATE SET _skill2.name = \"Java\",_skill2.level = \"expert\"', 'MERGE (_skill3:Skill{id:\\'_linux\\'}) ON CREATE SET _skill3.name = \"Linux\",_skill3.level = \"expert\"', 'MERGE (_skill4:Skill{id:\\'_splunk\\'}) ON CREATE SET _skill4.name = \"Splunk\",_skill4.level = \"expert\"', 'MERGE (_skill5:Skill{id:\\'_frontend\\'}) ON CREATE SET _skill5.name = \"front end\",_skill5.level = \"expert\"', 'MERGE (_skill6:Skill{id:\\'_db2\\'}) ON CREATE SET _skill6.name = \"Db2\",_skill6.level = \"expert\"', 'MERGE (_skill7:Skill{id:\\'_html5\\'}) ON CREATE SET _skill7.name = \"HTML5\",_skill7.level = \"expert\"', 'MERGE (_skill8:Skill{id:\\'_css3\\'}) ON CREATE SET _skill8.name = \"CSS3\",_skill8.level = \"expert\"', 'MERGE (_skill9:Skill{id:\\'_xml\\'}) ON CREATE SET _skill9.name = \"XML\",_skill9.level = \"expert\"', 'MERGE (_skill10:Skill{id:\\'_json\\'}) ON CREATE SET _skill10.name = \"JSON\",_skill10.level = \"expert\"', 'MERGE (_skill11:Skill{id:\\'_javascript\\'}) ON CREATE SET _skill11.name = \"JavaScript\",_skill11.level = \"expert\"', 'MERGE (_skill12:Skill{id:\\'_nodejs\\'}) ON CREATE SET _skill12.name = \"node.js\",_skill12.level = \"expert\"', 'MERGE (_skill13:Skill{id:\\'_npm\\'}) ON CREATE SET _skill13.name = \"NPM\",_skill13.level = \"expert\"', 'MERGE (_skill14:Skill{id:\\'_git\\'}) ON CREATE SET _skill14.name = \"GIT\",_skill14.level = \"expert\"', 'MERGE (_skill15:Skill{id:\\'_expressjs\\'}) ON CREATE SET _skill15.name = \"express.js\",_skill15.level = \"expert\"', 'MERGE (_skill16:Skill{id:\\'_jquery\\'}) ON CREATE SET _skill16.name = \"jQuery\",_skill16.level = \"expert\"', 'MERGE (_skill17:Skill{id:\\'_angular\\'}) ON CREATE SET _skill17.name = \"Angular\",_skill17.level = \"expert\"', 'MERGE (_skill18:Skill{id:\\'_bootstrap\\'}) ON CREATE SET _skill18.name = \"Bootstrap\",_skill18.level = \"expert\"', 'MERGE (_skill19:Skill{id:\\'_restfulapi\\'}) ON CREATE SET _skill19.name = \"Restful API\",_skill19.level = \"expert\"', 'MERGE (_skill20:Skill{id:\\'_java\\'}) ON CREATE SET _skill20.name = \"JAVA\",_skill20.level = \"expert\"', 'MERGE (_skill21:Skill{id:\\'_j2ee\\'}) ON CREATE SET _skill21.name = \"J2EE\",_skill21.level = \"expert\"', 'MERGE (_skill22:Skill{id:\\'_php\\'}) ON CREATE SET _skill22.name = \"PHP\",_skill22.level = \"expert\"', 'MERGE (_skill23:Skill{id:\\'_windows\\'}) ON CREATE SET _skill23.name = \"Windows\",_skill23.level = \"expert\"', 'MERGE (_skill24:Skill{id:\\'_macos\\'}) ON CREATE SET _skill24.name = \"Mac OS\",_skill24.level = \"expert\"', 'MERGE (_skill25:Skill{id:\\'_linux\\'}) ON CREATE SET _skill25.name = \"Linux\",_skill25.level = \"expert\"', 'MERGE (_skill26:Skill{id:\\'_ubuntu\\'}) ON CREATE SET _skill26.name = \"Ubuntu\",_skill26.level = \"expert\"', 'MERGE (_skill27:Skill{id:\\'_mint\\'}) ON CREATE SET _skill27.name = \"Mint\",_skill27.level = \"expert\"', 'MERGE (_skill28:Skill{id:\\'_kali\\'}) ON CREATE SET _skill28.name = \"Kali\",_skill28.level = \"expert\"', 'MERGE (_education1:Education{id:\\'emy_cv_1\\'}) ON CREATE SET _education1.degree = \"Bachelor of Arts\",_education1.university = \"THE UNIVERSITY OF THAI CHAMBER OF COMMERCE - BANGKOK, TH\",_education1.graduationDate = \"May 2001\",_education1.score = \"0.0\"', 'MERGE (_education2:Education{id:\\'emy_cv_2\\'}) ON CREATE SET _education2.degree = \"Master of Business Administration\",_education2.university = \"AMERICAN INTER CONTINENTAL UNIVERSITY ATLANTA\",_education2.graduationDate = \"December 2005\",_education2.score = \"0.0\"', 'MERGE (_education3:Education{id:\\'emy_cv_3\\'}) ON CREATE SET _education3.degree = \"Master of Science in Information Technology\",_education3.university = \"KENNESAW STATE UNIVERSITY - Kennesaw, GA\",_education3.graduationDate = \"May 2015\",_education3.score = \"0.0\"'] [\"\\n      MATCH (_position1:Position{id:'jmy_cv_1'})\\n      MATCH (_company1:Company{id:'_tataconsultantcyser'})\\n      MERGE (_position1)-[:AT_COMPANY]->(_company1)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_position1:Position{id:'jmy_cv_1'})\\n      MERGE (_person1)-[:HAS_POSITION]->(_position1)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill1:Skill{id:'_sql'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill1)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill2:Skill{id:'_java'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill2)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill3:Skill{id:'_linux'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill3)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill4:Skill{id:'_splunk'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill4)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill5:Skill{id:'_frontend'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill5)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill6:Skill{id:'_db2'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill6)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill7:Skill{id:'_html5'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill7)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill8:Skill{id:'_css3'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill8)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill9:Skill{id:'_xml'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill9)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill10:Skill{id:'_json'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill10)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill11:Skill{id:'_javascript'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill11)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill12:Skill{id:'_nodejs'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill12)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill13:Skill{id:'_npm'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill13)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill14:Skill{id:'_git'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill14)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill15:Skill{id:'_expressjs'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill15)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill16:Skill{id:'_jquery'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill16)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill17:Skill{id:'_angular'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill17)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill18:Skill{id:'_bootstrap'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill18)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill19:Skill{id:'_restfulapi'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill19)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill20:Skill{id:'_java'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill20)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill21:Skill{id:'_j2ee'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill21)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill22:Skill{id:'_php'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill22)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill23:Skill{id:'_windows'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill23)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill24:Skill{id:'_macos'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill24)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill25:Skill{id:'_linux'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill25)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill26:Skill{id:'_ubuntu'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill26)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill27:Skill{id:'_mint'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill27)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_skill28:Skill{id:'_kali'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill28)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_education1:Education{id:'emy_cv_1'})\\n      MERGE (_person1)-[:HAS_EDUCATION]->(_education1)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_education2:Education{id:'emy_cv_2'})\\n      MERGE (_person1)-[:HAS_EDUCATION]->(_education2)\\n    \", \"\\n      MATCH (_person1:Person{id:'pmy_cv'})\\n      MATCH (_education3:Education{id:'emy_cv_3'})\\n      MERGE (_person1)-[:HAS_EDUCATION]->(_education3)\\n    \"]\n"
     ]
    }
   ],
   "source": [
    "ent_cyp, rel_cyp = generate_cypher('my_cv', [results])\n",
    "\n",
    "print(ent_cyp, rel_cyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ru6fjsO8uTEM",
    "outputId": "4c786fd1-5691-4b11-e773-765e8dfd3ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-5.17.0.tar.gz (197 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/197.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m194.6/197.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n",
      "Building wheels for collected packages: neo4j\n",
      "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for neo4j: filename=neo4j-5.17.0-py3-none-any.whl size=273834 sha256=2f8eb55ba7ff84516554df34430295e1061fdec2f3b4b7f1710e395c9310a665\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/a1/15/63d729065b1a6a8afce3343003ca05bdbed2c4c05a707da4a3\n",
      "Successfully built neo4j\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-5.17.0\n"
     ]
    }
   ],
   "source": [
    "pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-2ImHNqdS52B"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3anWCFkTuQBF"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "# You will need to change these variables\n",
    "connectionUrl = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"Pratikps1$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "3j6TvTrqvhGA",
    "outputId": "f1604410-2f38-4b12-8def-9205982b4f40"
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(connectionUrl, auth=(username, password))\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C4va51vQv0ro"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run_query(query, params={}):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_query('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_position_id IF NOT EXISTS FOR (n:Position) REQUIRE (n.id) IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_skill_id IF NOT EXISTS FOR (n:Skill) REQUIRE n.id IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_education_id IF NOT EXISTS FOR (n:Education) REQUIRE n.id IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in ent_cyp:\n",
    "    run_query(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for r in rel_cyp:\n",
    "    run_query(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from string import Template\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def run_pipeline(start=0, count=1):\n",
    "    txt_files = glob.glob(\"data/*.txt\")[start:count]\n",
    "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
    "    failed_files = process_pipeline(txt_files)\n",
    "    print(failed_files)\n",
    "    return failed_files\n",
    "\n",
    "def process_pipeline(files):\n",
    "    failed_files = []\n",
    "    i = 0\n",
    "    for f in files:\n",
    "        i += 1\n",
    "        try:\n",
    "            with open(f, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                print(f\"  {f}: Reading File No. ({i})\")\n",
    "                data = file.read().rstrip()\n",
    "                text = data\n",
    "                print(f\"    {f}: Extracting Entities & Relationships\")\n",
    "                results = run_extraction(f, text)\n",
    "                print(f\"    {f}: Generating Cypher\")\n",
    "                ent_cyp, rel_cyp = generate_cypher(Path(f).stem, results)\n",
    "                print(f\"    {f}: Ingesting Entities\")\n",
    "                for e in ent_cyp:\n",
    "                    run_query(e)\n",
    "                print(f\"    {f}: Ingesting Relationships\")\n",
    "                for r in rel_cyp:\n",
    "                    run_query(r)\n",
    "                print(f\"    {f}: Processing DONE\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {f}: Processing Failed with exception {e}\")\n",
    "            failed_files.append(f)\n",
    "    return failed_files\n",
    "        \n",
    "from timeit import default_timer as timer\n",
    "def run_extraction(f, text):\n",
    "    start = timer()\n",
    "    prompts = [person_prompt_tpl, postion_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "    results = {\"entities\": [], \"relationships\": []}\n",
    "    for p in prompts:\n",
    "        _prompt = Template(p).substitute(ctext=text)\n",
    "        _extraction = extract_entities_relationships(_prompt, '')\n",
    "        if 'Answer:\\n' in _extraction:\n",
    "            _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "        if _extraction.strip() == '':\n",
    "            continue\n",
    "        try:\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        except json.JSONDecodeError:\n",
    "            #Temp hack to ignore Skills cut off by token limitation\n",
    "            _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "        if \"relationships\" in _extraction:\n",
    "            results[\"relationships\"].extend(_extraction[\"relationships\"])\n",
    "    person_id = results[\"entities\"][0][\"id\"]\n",
    "    for e in results[\"entities\"][1:]:\n",
    "        if e['label'] == 'Position':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "        if e['label'] == 'Skill':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "        if e['label'] == 'Education':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")\n",
    "    end = timer()\n",
    "    elapsed = (end-start)\n",
    "    print(f\"    {f}: Entity Extraction took {elapsed}secs\")\n",
    "    return [results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 100 files\n",
      "  data\\05499.txt: Reading File No. (1)\n",
      "    data\\05499.txt: Extracting Entities & Relationships\n",
      "    data\\05499.txt: Entity Extraction took 6.253810700029135secs\n",
      "    data\\05499.txt: Generating Cypher\n",
      "    data\\05499.txt: Ingesting Entities\n",
      "    data\\05499.txt: Ingesting Relationships\n",
      "    data\\05499.txt: Processing DONE\n",
      "  data\\05500.txt: Reading File No. (2)\n",
      "    data\\05500.txt: Extracting Entities & Relationships\n",
      "    data\\05500.txt: Entity Extraction took 5.341489000013098secs\n",
      "    data\\05500.txt: Generating Cypher\n",
      "    data\\05500.txt: Ingesting Entities\n",
      "    data\\05500.txt: Ingesting Relationships\n",
      "    data\\05500.txt: Processing DONE\n",
      "  data\\05501.txt: Reading File No. (3)\n",
      "    data\\05501.txt: Extracting Entities & Relationships\n",
      "    data\\05501.txt: Entity Extraction took 7.596793300006539secs\n",
      "    data\\05501.txt: Generating Cypher\n",
      "    data\\05501.txt: Ingesting Entities\n",
      "    data\\05501.txt: Ingesting Relationships\n",
      "    data\\05501.txt: Processing DONE\n",
      "  data\\05502.txt: Reading File No. (4)\n",
      "    data\\05502.txt: Extracting Entities & Relationships\n",
      "    data\\05502.txt: Entity Extraction took 9.798177300021052secs\n",
      "    data\\05502.txt: Generating Cypher\n",
      "    data\\05502.txt: Ingesting Entities\n",
      "    data\\05502.txt: Ingesting Relationships\n",
      "    data\\05502.txt: Processing DONE\n",
      "  data\\05503.txt: Reading File No. (5)\n",
      "    data\\05503.txt: Extracting Entities & Relationships\n",
      "    data\\05503.txt: Entity Extraction took 10.254760699812323secs\n",
      "    data\\05503.txt: Generating Cypher\n",
      "    data\\05503.txt: Ingesting Entities\n",
      "    data\\05503.txt: Ingesting Relationships\n",
      "    data\\05503.txt: Processing DONE\n",
      "  data\\05504.txt: Reading File No. (6)\n",
      "    data\\05504.txt: Extracting Entities & Relationships\n",
      "    data\\05504.txt: Entity Extraction took 6.72805740009062secs\n",
      "    data\\05504.txt: Generating Cypher\n",
      "    data\\05504.txt: Ingesting Entities\n",
      "    data\\05504.txt: Ingesting Relationships\n",
      "    data\\05504.txt: Processing DONE\n",
      "  data\\05505.txt: Reading File No. (7)\n",
      "    data\\05505.txt: Extracting Entities & Relationships\n",
      "    data\\05505.txt: Entity Extraction took 6.572890100069344secs\n",
      "    data\\05505.txt: Generating Cypher\n",
      "    data\\05505.txt: Ingesting Entities\n",
      "    data\\05505.txt: Ingesting Relationships\n",
      "    data\\05505.txt: Processing DONE\n",
      "  data\\05506.txt: Reading File No. (8)\n",
      "    data\\05506.txt: Extracting Entities & Relationships\n",
      "    data\\05506.txt: Entity Extraction took 9.60030180006288secs\n",
      "    data\\05506.txt: Generating Cypher\n",
      "    data\\05506.txt: Ingesting Entities\n",
      "    data\\05506.txt: Ingesting Relationships\n",
      "    data\\05506.txt: Processing DONE\n",
      "  data\\05507.txt: Reading File No. (9)\n",
      "    data\\05507.txt: Extracting Entities & Relationships\n",
      "    data\\05507.txt: Entity Extraction took 13.260094200028107secs\n",
      "    data\\05507.txt: Generating Cypher\n",
      "    data\\05507.txt: Ingesting Entities\n",
      "    data\\05507.txt: Ingesting Relationships\n",
      "    data\\05507.txt: Processing DONE\n",
      "  data\\05508.txt: Reading File No. (10)\n",
      "    data\\05508.txt: Extracting Entities & Relationships\n",
      "    data\\05508.txt: Entity Extraction took 5.2828224000986665secs\n",
      "    data\\05508.txt: Generating Cypher\n",
      "    data\\05508.txt: Ingesting Entities\n",
      "    data\\05508.txt: Ingesting Relationships\n",
      "    data\\05508.txt: Processing DONE\n",
      "  data\\05509.txt: Reading File No. (11)\n",
      "    data\\05509.txt: Extracting Entities & Relationships\n",
      "    data\\05509.txt: Entity Extraction took 4.905662700068206secs\n",
      "    data\\05509.txt: Generating Cypher\n",
      "    data\\05509.txt: Ingesting Entities\n",
      "    data\\05509.txt: Ingesting Relationships\n",
      "    data\\05509.txt: Processing DONE\n",
      "  data\\05510.txt: Reading File No. (12)\n",
      "    data\\05510.txt: Extracting Entities & Relationships\n",
      "    data\\05510.txt: Entity Extraction took 5.2201733000110835secs\n",
      "    data\\05510.txt: Generating Cypher\n",
      "    data\\05510.txt: Ingesting Entities\n",
      "    data\\05510.txt: Ingesting Relationships\n",
      "    data\\05510.txt: Processing DONE\n",
      "  data\\05511.txt: Reading File No. (13)\n",
      "    data\\05511.txt: Extracting Entities & Relationships\n",
      "    data\\05511.txt: Entity Extraction took 5.847491000080481secs\n",
      "    data\\05511.txt: Generating Cypher\n",
      "    data\\05511.txt: Ingesting Entities\n",
      "    data\\05511.txt: Ingesting Relationships\n",
      "    data\\05511.txt: Processing DONE\n",
      "  data\\05512.txt: Reading File No. (14)\n",
      "    data\\05512.txt: Extracting Entities & Relationships\n",
      "    data\\05512.txt: Entity Extraction took 5.756339299958199secs\n",
      "    data\\05512.txt: Generating Cypher\n",
      "    data\\05512.txt: Ingesting Entities\n",
      "    data\\05512.txt: Ingesting Relationships\n",
      "    data\\05512.txt: Processing DONE\n",
      "  data\\05513.txt: Reading File No. (15)\n",
      "    data\\05513.txt: Extracting Entities & Relationships\n",
      "    data\\05513.txt: Entity Extraction took 6.50105580012314secs\n",
      "    data\\05513.txt: Generating Cypher\n",
      "    data\\05513.txt: Ingesting Entities\n",
      "    data\\05513.txt: Ingesting Relationships\n",
      "    data\\05513.txt: Processing DONE\n",
      "  data\\05514.txt: Reading File No. (16)\n",
      "    data\\05514.txt: Extracting Entities & Relationships\n",
      "    data\\05514.txt: Processing Failed with exception Expecting ',' delimiter: line 1 column 1237 (char 1236)\n",
      "  data\\05515.txt: Reading File No. (17)\n",
      "    data\\05515.txt: Extracting Entities & Relationships\n",
      "    data\\05515.txt: Entity Extraction took 5.153576700016856secs\n",
      "    data\\05515.txt: Generating Cypher\n",
      "    data\\05515.txt: Ingesting Entities\n",
      "    data\\05515.txt: Ingesting Relationships\n",
      "    data\\05515.txt: Processing DONE\n",
      "  data\\05516.txt: Reading File No. (18)\n",
      "    data\\05516.txt: Extracting Entities & Relationships\n",
      "    data\\05516.txt: Entity Extraction took 6.991460700053722secs\n",
      "    data\\05516.txt: Generating Cypher\n",
      "    data\\05516.txt: Ingesting Entities\n",
      "    data\\05516.txt: Ingesting Relationships\n",
      "    data\\05516.txt: Processing DONE\n",
      "  data\\05517.txt: Reading File No. (19)\n",
      "    data\\05517.txt: Extracting Entities & Relationships\n",
      "    data\\05517.txt: Entity Extraction took 6.468631100142375secs\n",
      "    data\\05517.txt: Generating Cypher\n",
      "    data\\05517.txt: Ingesting Entities\n",
      "    data\\05517.txt: Ingesting Relationships\n",
      "    data\\05517.txt: Processing DONE\n",
      "  data\\05518.txt: Reading File No. (20)\n",
      "    data\\05518.txt: Extracting Entities & Relationships\n",
      "    data\\05518.txt: Entity Extraction took 4.798651000019163secs\n",
      "    data\\05518.txt: Generating Cypher\n",
      "    data\\05518.txt: Ingesting Entities\n",
      "    data\\05518.txt: Ingesting Relationships\n",
      "    data\\05518.txt: Processing DONE\n",
      "  data\\05519.txt: Reading File No. (21)\n",
      "    data\\05519.txt: Extracting Entities & Relationships\n",
      "    data\\05519.txt: Entity Extraction took 5.433187199989334secs\n",
      "    data\\05519.txt: Generating Cypher\n",
      "    data\\05519.txt: Ingesting Entities\n",
      "    data\\05519.txt: Ingesting Relationships\n",
      "    data\\05519.txt: Processing DONE\n",
      "  data\\05520.txt: Reading File No. (22)\n",
      "    data\\05520.txt: Extracting Entities & Relationships\n",
      "    data\\05520.txt: Entity Extraction took 6.1860095001757145secs\n",
      "    data\\05520.txt: Generating Cypher\n",
      "    data\\05520.txt: Ingesting Entities\n",
      "    data\\05520.txt: Ingesting Relationships\n",
      "    data\\05520.txt: Processing DONE\n",
      "  data\\05521.txt: Reading File No. (23)\n",
      "    data\\05521.txt: Extracting Entities & Relationships\n",
      "    data\\05521.txt: Entity Extraction took 5.7653730001766235secs\n",
      "    data\\05521.txt: Generating Cypher\n",
      "    data\\05521.txt: Ingesting Entities\n",
      "    data\\05521.txt: Ingesting Relationships\n",
      "    data\\05521.txt: Processing DONE\n",
      "  data\\05522.txt: Reading File No. (24)\n",
      "    data\\05522.txt: Extracting Entities & Relationships\n",
      "    data\\05522.txt: Entity Extraction took 5.264335799962282secs\n",
      "    data\\05522.txt: Generating Cypher\n",
      "    data\\05522.txt: Ingesting Entities\n",
      "    data\\05522.txt: Ingesting Relationships\n",
      "    data\\05522.txt: Processing DONE\n",
      "  data\\05523.txt: Reading File No. (25)\n",
      "    data\\05523.txt: Extracting Entities & Relationships\n",
      "    data\\05523.txt: Entity Extraction took 11.120557600166649secs\n",
      "    data\\05523.txt: Generating Cypher\n",
      "    data\\05523.txt: Ingesting Entities\n",
      "    data\\05523.txt: Ingesting Relationships\n",
      "    data\\05523.txt: Processing DONE\n",
      "  data\\05524.txt: Reading File No. (26)\n",
      "    data\\05524.txt: Extracting Entities & Relationships\n",
      "    data\\05524.txt: Entity Extraction took 8.772622700082138secs\n",
      "    data\\05524.txt: Generating Cypher\n",
      "    data\\05524.txt: Ingesting Entities\n",
      "    data\\05524.txt: Ingesting Relationships\n",
      "    data\\05524.txt: Processing DONE\n",
      "  data\\05525.txt: Reading File No. (27)\n",
      "    data\\05525.txt: Extracting Entities & Relationships\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data\\05525.txt: Entity Extraction took 9.269567699870095secs\n",
      "    data\\05525.txt: Generating Cypher\n",
      "    data\\05525.txt: Ingesting Entities\n",
      "    data\\05525.txt: Ingesting Relationships\n",
      "    data\\05525.txt: Processing DONE\n",
      "  data\\05526.txt: Reading File No. (28)\n",
      "    data\\05526.txt: Extracting Entities & Relationships\n",
      "    data\\05526.txt: Entity Extraction took 4.963630199898034secs\n",
      "    data\\05526.txt: Generating Cypher\n",
      "    data\\05526.txt: Ingesting Entities\n",
      "    data\\05526.txt: Ingesting Relationships\n",
      "    data\\05526.txt: Processing DONE\n",
      "  data\\05527.txt: Reading File No. (29)\n",
      "    data\\05527.txt: Extracting Entities & Relationships\n",
      "    data\\05527.txt: Entity Extraction took 3.2011431001592427secs\n",
      "    data\\05527.txt: Generating Cypher\n",
      "    data\\05527.txt: Ingesting Entities\n",
      "    data\\05527.txt: Ingesting Relationships\n",
      "    data\\05527.txt: Processing DONE\n",
      "  data\\05528.txt: Reading File No. (30)\n",
      "    data\\05528.txt: Extracting Entities & Relationships\n",
      "    data\\05528.txt: Entity Extraction took 5.89629269996658secs\n",
      "    data\\05528.txt: Generating Cypher\n",
      "    data\\05528.txt: Ingesting Entities\n",
      "    data\\05528.txt: Ingesting Relationships\n",
      "    data\\05528.txt: Processing DONE\n",
      "  data\\05529.txt: Reading File No. (31)\n",
      "    data\\05529.txt: Extracting Entities & Relationships\n",
      "    data\\05529.txt: Entity Extraction took 4.675443499814719secs\n",
      "    data\\05529.txt: Generating Cypher\n",
      "    data\\05529.txt: Ingesting Entities\n",
      "    data\\05529.txt: Ingesting Relationships\n",
      "    data\\05529.txt: Processing DONE\n",
      "  data\\05530.txt: Reading File No. (32)\n",
      "    data\\05530.txt: Extracting Entities & Relationships\n",
      "    data\\05530.txt: Entity Extraction took 4.910455200122669secs\n",
      "    data\\05530.txt: Generating Cypher\n",
      "    data\\05530.txt: Ingesting Entities\n",
      "    data\\05530.txt: Ingesting Relationships\n",
      "    data\\05530.txt: Processing DONE\n",
      "  data\\05531.txt: Reading File No. (33)\n",
      "    data\\05531.txt: Extracting Entities & Relationships\n",
      "    data\\05531.txt: Entity Extraction took 6.276413100073114secs\n",
      "    data\\05531.txt: Generating Cypher\n",
      "    data\\05531.txt: Ingesting Entities\n",
      "    data\\05531.txt: Ingesting Relationships\n",
      "    data\\05531.txt: Processing DONE\n",
      "  data\\05532.txt: Reading File No. (34)\n",
      "    data\\05532.txt: Extracting Entities & Relationships\n",
      "    data\\05532.txt: Entity Extraction took 5.736502799903974secs\n",
      "    data\\05532.txt: Generating Cypher\n",
      "    data\\05532.txt: Ingesting Entities\n",
      "    data\\05532.txt: Ingesting Relationships\n",
      "    data\\05532.txt: Processing DONE\n",
      "  data\\05533.txt: Reading File No. (35)\n",
      "    data\\05533.txt: Extracting Entities & Relationships\n",
      "    data\\05533.txt: Entity Extraction took 4.613583200145513secs\n",
      "    data\\05533.txt: Generating Cypher\n",
      "    data\\05533.txt: Ingesting Entities\n",
      "    data\\05533.txt: Ingesting Relationships\n",
      "    data\\05533.txt: Processing DONE\n",
      "  data\\05534.txt: Reading File No. (36)\n",
      "    data\\05534.txt: Extracting Entities & Relationships\n",
      "    data\\05534.txt: Entity Extraction took 13.904247200116515secs\n",
      "    data\\05534.txt: Generating Cypher\n",
      "    data\\05534.txt: Ingesting Entities\n",
      "    data\\05534.txt: Ingesting Relationships\n",
      "    data\\05534.txt: Processing DONE\n",
      "  data\\05535.txt: Reading File No. (37)\n",
      "    data\\05535.txt: Extracting Entities & Relationships\n",
      "    data\\05535.txt: Entity Extraction took 6.12459409981966secs\n",
      "    data\\05535.txt: Generating Cypher\n",
      "    data\\05535.txt: Ingesting Entities\n",
      "    data\\05535.txt: Ingesting Relationships\n",
      "    data\\05535.txt: Processing DONE\n",
      "  data\\05536.txt: Reading File No. (38)\n",
      "    data\\05536.txt: Extracting Entities & Relationships\n",
      "    data\\05536.txt: Entity Extraction took 4.039243099978194secs\n",
      "    data\\05536.txt: Generating Cypher\n",
      "    data\\05536.txt: Ingesting Entities\n",
      "    data\\05536.txt: Ingesting Relationships\n",
      "    data\\05536.txt: Processing DONE\n",
      "  data\\05537.txt: Reading File No. (39)\n",
      "    data\\05537.txt: Extracting Entities & Relationships\n",
      "    data\\05537.txt: Entity Extraction took 9.30500839999877secs\n",
      "    data\\05537.txt: Generating Cypher\n",
      "    data\\05537.txt: Ingesting Entities\n",
      "    data\\05537.txt: Ingesting Relationships\n",
      "    data\\05537.txt: Processing DONE\n",
      "  data\\05538.txt: Reading File No. (40)\n",
      "    data\\05538.txt: Extracting Entities & Relationships\n",
      "    data\\05538.txt: Entity Extraction took 5.802262500161305secs\n",
      "    data\\05538.txt: Generating Cypher\n",
      "    data\\05538.txt: Ingesting Entities\n",
      "    data\\05538.txt: Ingesting Relationships\n",
      "    data\\05538.txt: Processing DONE\n",
      "  data\\05539.txt: Reading File No. (41)\n",
      "    data\\05539.txt: Extracting Entities & Relationships\n",
      "    data\\05539.txt: Entity Extraction took 8.520779999904335secs\n",
      "    data\\05539.txt: Generating Cypher\n",
      "    data\\05539.txt: Ingesting Entities\n",
      "    data\\05539.txt: Ingesting Relationships\n",
      "    data\\05539.txt: Processing DONE\n",
      "  data\\05540.txt: Reading File No. (42)\n",
      "    data\\05540.txt: Extracting Entities & Relationships\n",
      "    data\\05540.txt: Entity Extraction took 12.550465400097892secs\n",
      "    data\\05540.txt: Generating Cypher\n",
      "    data\\05540.txt: Ingesting Entities\n",
      "    data\\05540.txt: Ingesting Relationships\n",
      "    data\\05540.txt: Processing DONE\n",
      "  data\\05541.txt: Reading File No. (43)\n",
      "    data\\05541.txt: Extracting Entities & Relationships\n",
      "    data\\05541.txt: Entity Extraction took 5.0578282999340445secs\n",
      "    data\\05541.txt: Generating Cypher\n",
      "    data\\05541.txt: Ingesting Entities\n",
      "    data\\05541.txt: Ingesting Relationships\n",
      "    data\\05541.txt: Processing DONE\n",
      "  data\\05542.txt: Reading File No. (44)\n",
      "    data\\05542.txt: Extracting Entities & Relationships\n",
      "    data\\05542.txt: Entity Extraction took 14.97863750020042secs\n",
      "    data\\05542.txt: Generating Cypher\n",
      "    data\\05542.txt: Ingesting Entities\n",
      "    data\\05542.txt: Ingesting Relationships\n",
      "    data\\05542.txt: Processing DONE\n",
      "  data\\05543.txt: Reading File No. (45)\n",
      "    data\\05543.txt: Extracting Entities & Relationships\n",
      "    data\\05543.txt: Entity Extraction took 8.52493069996126secs\n",
      "    data\\05543.txt: Generating Cypher\n",
      "    data\\05543.txt: Ingesting Entities\n",
      "    data\\05543.txt: Ingesting Relationships\n",
      "    data\\05543.txt: Processing DONE\n",
      "  data\\05544.txt: Reading File No. (46)\n",
      "    data\\05544.txt: Extracting Entities & Relationships\n",
      "    data\\05544.txt: Entity Extraction took 11.768929499899969secs\n",
      "    data\\05544.txt: Generating Cypher\n",
      "    data\\05544.txt: Ingesting Entities\n",
      "    data\\05544.txt: Ingesting Relationships\n",
      "    data\\05544.txt: Processing DONE\n",
      "  data\\05545.txt: Reading File No. (47)\n",
      "    data\\05545.txt: Extracting Entities & Relationships\n",
      "    data\\05545.txt: Entity Extraction took 7.704563000006601secs\n",
      "    data\\05545.txt: Generating Cypher\n",
      "    data\\05545.txt: Ingesting Entities\n",
      "    data\\05545.txt: Ingesting Relationships\n",
      "    data\\05545.txt: Processing DONE\n",
      "  data\\05546.txt: Reading File No. (48)\n",
      "    data\\05546.txt: Extracting Entities & Relationships\n",
      "    data\\05546.txt: Entity Extraction took 11.293330800021067secs\n",
      "    data\\05546.txt: Generating Cypher\n",
      "    data\\05546.txt: Ingesting Entities\n",
      "    data\\05546.txt: Ingesting Relationships\n",
      "    data\\05546.txt: Processing DONE\n",
      "  data\\05547.txt: Reading File No. (49)\n",
      "    data\\05547.txt: Extracting Entities & Relationships\n",
      "    data\\05547.txt: Entity Extraction took 10.62408269988373secs\n",
      "    data\\05547.txt: Generating Cypher\n",
      "    data\\05547.txt: Ingesting Entities\n",
      "    data\\05547.txt: Ingesting Relationships\n",
      "    data\\05547.txt: Processing DONE\n",
      "  data\\05548.txt: Reading File No. (50)\n",
      "    data\\05548.txt: Extracting Entities & Relationships\n",
      "    data\\05548.txt: Entity Extraction took 14.028423500014469secs\n",
      "    data\\05548.txt: Generating Cypher\n",
      "    data\\05548.txt: Ingesting Entities\n",
      "    data\\05548.txt: Ingesting Relationships\n",
      "    data\\05548.txt: Processing DONE\n",
      "  data\\05549.txt: Reading File No. (51)\n",
      "    data\\05549.txt: Extracting Entities & Relationships\n",
      "    data\\05549.txt: Entity Extraction took 12.355363400187343secs\n",
      "    data\\05549.txt: Generating Cypher\n",
      "    data\\05549.txt: Ingesting Entities\n",
      "    data\\05549.txt: Ingesting Relationships\n",
      "    data\\05549.txt: Processing DONE\n",
      "  data\\05550.txt: Reading File No. (52)\n",
      "    data\\05550.txt: Extracting Entities & Relationships\n",
      "    data\\05550.txt: Entity Extraction took 10.803662299877033secs\n",
      "    data\\05550.txt: Generating Cypher\n",
      "    data\\05550.txt: Ingesting Entities\n",
      "    data\\05550.txt: Ingesting Relationships\n",
      "    data\\05550.txt: Processing DONE\n",
      "  data\\05551.txt: Reading File No. (53)\n",
      "    data\\05551.txt: Extracting Entities & Relationships\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data\\05551.txt: Entity Extraction took 9.044277400011197secs\n",
      "    data\\05551.txt: Generating Cypher\n",
      "    data\\05551.txt: Ingesting Entities\n",
      "    data\\05551.txt: Ingesting Relationships\n",
      "    data\\05551.txt: Processing DONE\n",
      "  data\\05552.txt: Reading File No. (54)\n",
      "    data\\05552.txt: Extracting Entities & Relationships\n",
      "    data\\05552.txt: Entity Extraction took 5.577560199890286secs\n",
      "    data\\05552.txt: Generating Cypher\n",
      "    data\\05552.txt: Ingesting Entities\n",
      "    data\\05552.txt: Ingesting Relationships\n",
      "    data\\05552.txt: Processing DONE\n",
      "  data\\05553.txt: Reading File No. (55)\n",
      "    data\\05553.txt: Extracting Entities & Relationships\n",
      "    data\\05553.txt: Entity Extraction took 4.331262900028378secs\n",
      "    data\\05553.txt: Generating Cypher\n",
      "    data\\05553.txt: Ingesting Entities\n",
      "    data\\05553.txt: Ingesting Relationships\n",
      "    data\\05553.txt: Processing DONE\n",
      "  data\\05554.txt: Reading File No. (56)\n",
      "    data\\05554.txt: Extracting Entities & Relationships\n",
      "    data\\05554.txt: Entity Extraction took 4.734944500029087secs\n",
      "    data\\05554.txt: Generating Cypher\n",
      "    data\\05554.txt: Ingesting Entities\n",
      "    data\\05554.txt: Ingesting Relationships\n",
      "    data\\05554.txt: Processing DONE\n",
      "  data\\05555.txt: Reading File No. (57)\n",
      "    data\\05555.txt: Extracting Entities & Relationships\n",
      "    data\\05555.txt: Entity Extraction took 6.596117300214246secs\n",
      "    data\\05555.txt: Generating Cypher\n",
      "    data\\05555.txt: Ingesting Entities\n",
      "    data\\05555.txt: Ingesting Relationships\n",
      "    data\\05555.txt: Processing DONE\n",
      "  data\\05556.txt: Reading File No. (58)\n",
      "    data\\05556.txt: Extracting Entities & Relationships\n",
      "    data\\05556.txt: Entity Extraction took 5.23776250006631secs\n",
      "    data\\05556.txt: Generating Cypher\n",
      "    data\\05556.txt: Ingesting Entities\n",
      "    data\\05556.txt: Ingesting Relationships\n",
      "    data\\05556.txt: Processing DONE\n",
      "  data\\05557.txt: Reading File No. (59)\n",
      "    data\\05557.txt: Extracting Entities & Relationships\n",
      "    data\\05557.txt: Entity Extraction took 10.839642100036144secs\n",
      "    data\\05557.txt: Generating Cypher\n",
      "    data\\05557.txt: Ingesting Entities\n",
      "    data\\05557.txt: Ingesting Relationships\n",
      "    data\\05557.txt: Processing DONE\n",
      "  data\\05558.txt: Reading File No. (60)\n",
      "    data\\05558.txt: Extracting Entities & Relationships\n",
      "    data\\05558.txt: Entity Extraction took 5.217184199951589secs\n",
      "    data\\05558.txt: Generating Cypher\n",
      "    data\\05558.txt: Ingesting Entities\n",
      "    data\\05558.txt: Ingesting Relationships\n",
      "    data\\05558.txt: Processing DONE\n",
      "  data\\05559.txt: Reading File No. (61)\n",
      "    data\\05559.txt: Extracting Entities & Relationships\n",
      "    data\\05559.txt: Entity Extraction took 5.503096800064668secs\n",
      "    data\\05559.txt: Generating Cypher\n",
      "    data\\05559.txt: Ingesting Entities\n",
      "    data\\05559.txt: Ingesting Relationships\n",
      "    data\\05559.txt: Processing DONE\n",
      "  data\\05560.txt: Reading File No. (62)\n",
      "    data\\05560.txt: Extracting Entities & Relationships\n",
      "    data\\05560.txt: Entity Extraction took 7.510653299978003secs\n",
      "    data\\05560.txt: Generating Cypher\n",
      "    data\\05560.txt: Ingesting Entities\n",
      "    data\\05560.txt: Ingesting Relationships\n",
      "    data\\05560.txt: Processing DONE\n",
      "  data\\05561.txt: Reading File No. (63)\n",
      "    data\\05561.txt: Extracting Entities & Relationships\n",
      "    data\\05561.txt: Entity Extraction took 6.782618299825117secs\n",
      "    data\\05561.txt: Generating Cypher\n",
      "    data\\05561.txt: Ingesting Entities\n",
      "    data\\05561.txt: Ingesting Relationships\n",
      "    data\\05561.txt: Processing DONE\n",
      "  data\\05562.txt: Reading File No. (64)\n",
      "    data\\05562.txt: Extracting Entities & Relationships\n",
      "    data\\05562.txt: Entity Extraction took 16.71747409994714secs\n",
      "    data\\05562.txt: Generating Cypher\n",
      "    data\\05562.txt: Processing Failed with exception 'name'\n",
      "  data\\05563.txt: Reading File No. (65)\n",
      "    data\\05563.txt: Extracting Entities & Relationships\n",
      "    data\\05563.txt: Entity Extraction took 8.942934500053525secs\n",
      "    data\\05563.txt: Generating Cypher\n",
      "    data\\05563.txt: Ingesting Entities\n",
      "    data\\05563.txt: Ingesting Relationships\n",
      "    data\\05563.txt: Processing DONE\n",
      "  data\\05564.txt: Reading File No. (66)\n",
      "    data\\05564.txt: Extracting Entities & Relationships\n",
      "    data\\05564.txt: Entity Extraction took 9.892779499990866secs\n",
      "    data\\05564.txt: Generating Cypher\n",
      "    data\\05564.txt: Ingesting Entities\n",
      "    data\\05564.txt: Ingesting Relationships\n",
      "    data\\05564.txt: Processing DONE\n",
      "  data\\05565.txt: Reading File No. (67)\n",
      "    data\\05565.txt: Extracting Entities & Relationships\n",
      "    data\\05565.txt: Entity Extraction took 6.322569400072098secs\n",
      "    data\\05565.txt: Generating Cypher\n",
      "    data\\05565.txt: Ingesting Entities\n",
      "    data\\05565.txt: Ingesting Relationships\n",
      "    data\\05565.txt: Processing DONE\n",
      "  data\\05566.txt: Reading File No. (68)\n",
      "    data\\05566.txt: Extracting Entities & Relationships\n",
      "    data\\05566.txt: Entity Extraction took 5.732436599908397secs\n",
      "    data\\05566.txt: Generating Cypher\n",
      "    data\\05566.txt: Ingesting Entities\n",
      "    data\\05566.txt: Ingesting Relationships\n",
      "    data\\05566.txt: Processing DONE\n",
      "  data\\05567.txt: Reading File No. (69)\n",
      "    data\\05567.txt: Extracting Entities & Relationships\n",
      "    data\\05567.txt: Entity Extraction took 13.378253000089899secs\n",
      "    data\\05567.txt: Generating Cypher\n",
      "    data\\05567.txt: Ingesting Entities\n",
      "    data\\05567.txt: Ingesting Relationships\n",
      "    data\\05567.txt: Processing DONE\n",
      "  data\\05568.txt: Reading File No. (70)\n",
      "    data\\05568.txt: Extracting Entities & Relationships\n",
      "    data\\05568.txt: Entity Extraction took 7.582317600026727secs\n",
      "    data\\05568.txt: Generating Cypher\n",
      "    data\\05568.txt: Ingesting Entities\n",
      "    data\\05568.txt: Ingesting Relationships\n",
      "    data\\05568.txt: Processing DONE\n",
      "  data\\05569.txt: Reading File No. (71)\n",
      "    data\\05569.txt: Extracting Entities & Relationships\n",
      "    data\\05569.txt: Entity Extraction took 13.600711800158024secs\n",
      "    data\\05569.txt: Generating Cypher\n",
      "    data\\05569.txt: Ingesting Entities\n",
      "    data\\05569.txt: Ingesting Relationships\n",
      "    data\\05569.txt: Processing DONE\n",
      "  data\\05570.txt: Reading File No. (72)\n",
      "    data\\05570.txt: Extracting Entities & Relationships\n",
      "    data\\05570.txt: Entity Extraction took 11.792046400019899secs\n",
      "    data\\05570.txt: Generating Cypher\n",
      "    data\\05570.txt: Ingesting Entities\n",
      "    data\\05570.txt: Ingesting Relationships\n",
      "    data\\05570.txt: Processing DONE\n",
      "  data\\05571.txt: Reading File No. (73)\n",
      "    data\\05571.txt: Extracting Entities & Relationships\n",
      "    data\\05571.txt: Entity Extraction took 12.029574100160971secs\n",
      "    data\\05571.txt: Generating Cypher\n",
      "    data\\05571.txt: Ingesting Entities\n",
      "    data\\05571.txt: Ingesting Relationships\n",
      "    data\\05571.txt: Processing DONE\n",
      "  data\\05572.txt: Reading File No. (74)\n",
      "    data\\05572.txt: Extracting Entities & Relationships\n",
      "    data\\05572.txt: Entity Extraction took 6.233066400047392secs\n",
      "    data\\05572.txt: Generating Cypher\n",
      "    data\\05572.txt: Ingesting Entities\n",
      "    data\\05572.txt: Ingesting Relationships\n",
      "    data\\05572.txt: Processing DONE\n",
      "  data\\05573.txt: Reading File No. (75)\n",
      "    data\\05573.txt: Extracting Entities & Relationships\n",
      "    data\\05573.txt: Entity Extraction took 10.762194599956274secs\n",
      "    data\\05573.txt: Generating Cypher\n",
      "    data\\05573.txt: Ingesting Entities\n",
      "    data\\05573.txt: Ingesting Relationships\n",
      "    data\\05573.txt: Processing DONE\n",
      "  data\\05574.txt: Reading File No. (76)\n",
      "    data\\05574.txt: Extracting Entities & Relationships\n",
      "    data\\05574.txt: Entity Extraction took 6.7488633000757545secs\n",
      "    data\\05574.txt: Generating Cypher\n",
      "    data\\05574.txt: Ingesting Entities\n",
      "    data\\05574.txt: Ingesting Relationships\n",
      "    data\\05574.txt: Processing DONE\n",
      "  data\\05575.txt: Reading File No. (77)\n",
      "    data\\05575.txt: Extracting Entities & Relationships\n",
      "    data\\05575.txt: Entity Extraction took 9.610550499986857secs\n",
      "    data\\05575.txt: Generating Cypher\n",
      "    data\\05575.txt: Ingesting Entities\n",
      "    data\\05575.txt: Ingesting Relationships\n",
      "    data\\05575.txt: Processing DONE\n",
      "  data\\05576.txt: Reading File No. (78)\n",
      "    data\\05576.txt: Extracting Entities & Relationships\n",
      "    data\\05576.txt: Entity Extraction took 9.359410600038245secs\n",
      "    data\\05576.txt: Generating Cypher\n",
      "    data\\05576.txt: Ingesting Entities\n",
      "    data\\05576.txt: Ingesting Relationships\n",
      "    data\\05576.txt: Processing DONE\n",
      "  data\\05577.txt: Reading File No. (79)\n",
      "    data\\05577.txt: Extracting Entities & Relationships\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    data\\05577.txt: Entity Extraction took 7.329631999833509secs\n",
      "    data\\05577.txt: Generating Cypher\n",
      "    data\\05577.txt: Ingesting Entities\n",
      "    data\\05577.txt: Ingesting Relationships\n",
      "    data\\05577.txt: Processing DONE\n",
      "  data\\05578.txt: Reading File No. (80)\n",
      "    data\\05578.txt: Extracting Entities & Relationships\n",
      "    data\\05578.txt: Entity Extraction took 6.288914200151339secs\n",
      "    data\\05578.txt: Generating Cypher\n",
      "    data\\05578.txt: Ingesting Entities\n",
      "    data\\05578.txt: Ingesting Relationships\n",
      "    data\\05578.txt: Processing DONE\n",
      "  data\\05579.txt: Reading File No. (81)\n",
      "    data\\05579.txt: Extracting Entities & Relationships\n",
      "    data\\05579.txt: Entity Extraction took 12.537258799886331secs\n",
      "    data\\05579.txt: Generating Cypher\n",
      "    data\\05579.txt: Ingesting Entities\n",
      "    data\\05579.txt: Ingesting Relationships\n",
      "    data\\05579.txt: Processing DONE\n",
      "  data\\05580.txt: Reading File No. (82)\n",
      "    data\\05580.txt: Extracting Entities & Relationships\n",
      "    data\\05580.txt: Entity Extraction took 12.902897600084543secs\n",
      "    data\\05580.txt: Generating Cypher\n",
      "    data\\05580.txt: Ingesting Entities\n",
      "    data\\05580.txt: Ingesting Relationships\n",
      "    data\\05580.txt: Processing DONE\n",
      "  data\\05581.txt: Reading File No. (83)\n",
      "    data\\05581.txt: Extracting Entities & Relationships\n",
      "    data\\05581.txt: Entity Extraction took 6.111174099845812secs\n",
      "    data\\05581.txt: Generating Cypher\n",
      "    data\\05581.txt: Ingesting Entities\n",
      "    data\\05581.txt: Ingesting Relationships\n",
      "    data\\05581.txt: Processing DONE\n",
      "  data\\05582.txt: Reading File No. (84)\n",
      "    data\\05582.txt: Extracting Entities & Relationships\n",
      "    data\\05582.txt: Entity Extraction took 5.5096640000119805secs\n",
      "    data\\05582.txt: Generating Cypher\n",
      "    data\\05582.txt: Ingesting Entities\n",
      "    data\\05582.txt: Ingesting Relationships\n",
      "    data\\05582.txt: Processing DONE\n",
      "  data\\05583.txt: Reading File No. (85)\n",
      "    data\\05583.txt: Extracting Entities & Relationships\n",
      "    data\\05583.txt: Entity Extraction took 4.80895010009408secs\n",
      "    data\\05583.txt: Generating Cypher\n",
      "    data\\05583.txt: Ingesting Entities\n",
      "    data\\05583.txt: Ingesting Relationships\n",
      "    data\\05583.txt: Processing DONE\n",
      "  data\\05584.txt: Reading File No. (86)\n",
      "    data\\05584.txt: Extracting Entities & Relationships\n",
      "    data\\05584.txt: Entity Extraction took 6.102024199906737secs\n",
      "    data\\05584.txt: Generating Cypher\n",
      "    data\\05584.txt: Ingesting Entities\n",
      "    data\\05584.txt: Ingesting Relationships\n",
      "    data\\05584.txt: Processing DONE\n",
      "  data\\05585.txt: Reading File No. (87)\n",
      "    data\\05585.txt: Extracting Entities & Relationships\n",
      "    data\\05585.txt: Entity Extraction took 6.957712200004607secs\n",
      "    data\\05585.txt: Generating Cypher\n",
      "    data\\05585.txt: Ingesting Entities\n",
      "    data\\05585.txt: Ingesting Relationships\n",
      "    data\\05585.txt: Processing DONE\n",
      "  data\\05586.txt: Reading File No. (88)\n",
      "    data\\05586.txt: Extracting Entities & Relationships\n",
      "    data\\05586.txt: Entity Extraction took 6.105447399895638secs\n",
      "    data\\05586.txt: Generating Cypher\n",
      "    data\\05586.txt: Ingesting Entities\n",
      "    data\\05586.txt: Ingesting Relationships\n",
      "    data\\05586.txt: Processing DONE\n",
      "  data\\05587.txt: Reading File No. (89)\n",
      "    data\\05587.txt: Extracting Entities & Relationships\n",
      "    data\\05587.txt: Entity Extraction took 8.184736300026998secs\n",
      "    data\\05587.txt: Generating Cypher\n",
      "    data\\05587.txt: Ingesting Entities\n",
      "    data\\05587.txt: Ingesting Relationships\n",
      "    data\\05587.txt: Processing DONE\n",
      "  data\\05588.txt: Reading File No. (90)\n",
      "    data\\05588.txt: Extracting Entities & Relationships\n",
      "    data\\05588.txt: Entity Extraction took 6.749762800056487secs\n",
      "    data\\05588.txt: Generating Cypher\n",
      "    data\\05588.txt: Ingesting Entities\n",
      "    data\\05588.txt: Ingesting Relationships\n",
      "    data\\05588.txt: Processing DONE\n",
      "  data\\05589.txt: Reading File No. (91)\n",
      "    data\\05589.txt: Extracting Entities & Relationships\n",
      "    data\\05589.txt: Entity Extraction took 9.132479200139642secs\n",
      "    data\\05589.txt: Generating Cypher\n",
      "    data\\05589.txt: Ingesting Entities\n",
      "    data\\05589.txt: Ingesting Relationships\n",
      "    data\\05589.txt: Processing DONE\n",
      "  data\\05590.txt: Reading File No. (92)\n",
      "    data\\05590.txt: Extracting Entities & Relationships\n",
      "    data\\05590.txt: Entity Extraction took 5.701267800061032secs\n",
      "    data\\05590.txt: Generating Cypher\n",
      "    data\\05590.txt: Ingesting Entities\n",
      "    data\\05590.txt: Ingesting Relationships\n",
      "    data\\05590.txt: Processing DONE\n",
      "  data\\05591.txt: Reading File No. (93)\n",
      "    data\\05591.txt: Extracting Entities & Relationships\n",
      "    data\\05591.txt: Entity Extraction took 7.627308999886736secs\n",
      "    data\\05591.txt: Generating Cypher\n",
      "    data\\05591.txt: Ingesting Entities\n",
      "    data\\05591.txt: Ingesting Relationships\n",
      "    data\\05591.txt: Processing DONE\n",
      "  data\\05592.txt: Reading File No. (94)\n",
      "    data\\05592.txt: Extracting Entities & Relationships\n",
      "    data\\05592.txt: Entity Extraction took 6.121114099863917secs\n",
      "    data\\05592.txt: Generating Cypher\n",
      "    data\\05592.txt: Ingesting Entities\n",
      "    data\\05592.txt: Ingesting Relationships\n",
      "    data\\05592.txt: Processing DONE\n",
      "  data\\05593.txt: Reading File No. (95)\n",
      "    data\\05593.txt: Extracting Entities & Relationships\n",
      "    data\\05593.txt: Entity Extraction took 6.584509300068021secs\n",
      "    data\\05593.txt: Generating Cypher\n",
      "    data\\05593.txt: Ingesting Entities\n",
      "    data\\05593.txt: Ingesting Relationships\n",
      "    data\\05593.txt: Processing DONE\n",
      "  data\\05594.txt: Reading File No. (96)\n",
      "    data\\05594.txt: Extracting Entities & Relationships\n",
      "    data\\05594.txt: Entity Extraction took 5.716071600094438secs\n",
      "    data\\05594.txt: Generating Cypher\n",
      "    data\\05594.txt: Ingesting Entities\n",
      "    data\\05594.txt: Ingesting Relationships\n",
      "    data\\05594.txt: Processing DONE\n",
      "  data\\05595.txt: Reading File No. (97)\n",
      "    data\\05595.txt: Extracting Entities & Relationships\n",
      "    data\\05595.txt: Entity Extraction took 11.340059600071982secs\n",
      "    data\\05595.txt: Generating Cypher\n",
      "    data\\05595.txt: Ingesting Entities\n",
      "    data\\05595.txt: Ingesting Relationships\n",
      "    data\\05595.txt: Processing DONE\n",
      "  data\\05596.txt: Reading File No. (98)\n",
      "    data\\05596.txt: Extracting Entities & Relationships\n",
      "    data\\05596.txt: Entity Extraction took 10.505019799806178secs\n",
      "    data\\05596.txt: Generating Cypher\n",
      "    data\\05596.txt: Ingesting Entities\n",
      "    data\\05596.txt: Ingesting Relationships\n",
      "    data\\05596.txt: Processing DONE\n",
      "  data\\05597.txt: Reading File No. (99)\n",
      "    data\\05597.txt: Extracting Entities & Relationships\n",
      "    data\\05597.txt: Entity Extraction took 6.255315599963069secs\n",
      "    data\\05597.txt: Generating Cypher\n",
      "    data\\05597.txt: Ingesting Entities\n",
      "    data\\05597.txt: Ingesting Relationships\n",
      "    data\\05597.txt: Processing DONE\n",
      "  data\\05598.txt: Reading File No. (100)\n",
      "    data\\05598.txt: Extracting Entities & Relationships\n",
      "    data\\05598.txt: Entity Extraction took 6.041405300144106secs\n",
      "    data\\05598.txt: Generating Cypher\n",
      "    data\\05598.txt: Ingesting Entities\n",
      "    data\\05598.txt: Ingesting Relationships\n",
      "    data\\05598.txt: Processing DONE\n",
      "['data\\\\05514.txt', 'data\\\\05562.txt']\n"
     ]
    }
   ],
   "source": [
    "failed_files = run_pipeline(0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\05514.txt', 'data\\\\05562.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path=\"eng-to-cypher-trng.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resumeanz'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.oauth2.service_account.Credentials at 0x1fde0a08310>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241m.\u001b[39mClient(credential\u001b[38;5;241m=\u001b[39mcredentials)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'storage' is not defined"
     ]
    }
   ],
   "source": [
    "client = storage.Client(credential=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "bucket_name = project_id + '-genai'\n",
    "client = storage.Client(credentials=credentials)\n",
    "try:\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "except:\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.storage_class = 'STANDARD'\n",
    "    bucket = client.create_bucket(bucket)\n",
    "\n",
    "upload_name = f\"eng-to-cypher-trng-{timer()}.jsonl\" #this ensures vertexai reloads the file\n",
    "filename = 'eng-to-cypher-trng.jsonl'\n",
    "blob = bucket.blob(upload_name)\n",
    "blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'local_file_path' is the path to your local training data file in JSONL format\n",
    "training_data = 'gs://' + bucket_name + '/' + upload_name\n",
    "train_steps = 100\n",
    "\n",
    "# Initialize Vertex AI with the project and location\n",
    "vertexai.init(project=project_id,credentials=credentials)\n",
    "\n",
    "# Load a pre-trained text generation model\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "# Fine-tune the model with the local JSONL file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projects/209280395857/locations/us-central1/models/1718472902539476992']\n"
     ]
    }
   ],
   "source": [
    "tuned_model_names = model.list_tuned_model_names()\n",
    "print(tuned_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " tuned_model_name=\"projects/209280395857/locations/us-central1/models/1718472902539476992\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_cypher_text_bison(prompt, tuned_model_name = ''):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0.1, 1024, 0.95, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = \"\"\"\n",
    "Question: How many expert java developers attend more than one universities?\n",
    "Answer: MATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_EDUCATION]->(e1:Education), (p)-[:HAS_EDUCATION]->(e2:Education) WHERE toLower(s.name) CONTAINS 'java' AND toLower(s.level) CONTAINS 'expert' AND e1.university <> e2.university RETURN COUNT(DISTINCT p)\n",
    "\n",
    "Question: Where do most candidates get educated?\n",
    "Answer: MATCH (p:Person)-[:HAS_EDUCATION]->(e:Education) RETURN e.university, count(e.university) as alumni ORDER BY alumni DESC LIMIT 1\n",
    "\n",
    "Question: How many people have worked as a Data Scientist in San Francisco?\n",
    "Answer: MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.title) CONTAINS 'data scientist' AND toLower(pos.location) CONTAINS 'san francisco' RETURN COUNT(p)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Use only Nodes and relationships mentioned in the schema\n",
    "4. Always enclose the Cypher output inside 3 backticks\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "6. Candidate node is synonymous to Person\n",
    "7. Always use aliases to refer the node in the query\n",
    "8. Cypher is NOT SQL. So, do not mix and match the syntaxes\n",
    "Schema:\n",
    "(:Person {label: 'Person', id: string, role: string, description: string})-[:HAS_POSITION {}]->(:Position {label: 'Position', id: string, title: string, location: string, startDate: string, endDate: string, url: string})\n",
    "(:Position {label: 'Position', id: string, title: string, location: string, startDate: string, endDate: string, url: string})-[:AT_COMPANY {}]->(:Company {label:'Company', id: string, name: string})\n",
    "(:Person {label: 'Person',id: string, role: string, description: string})-[:HAS_SKILL {}]->(:Skill {label:'Skill', id: string,name: string,level: string})\n",
    "(:Person {label: 'Person',id: string, role: string, description: string})-[:HAS_EDUCATION {}]->(:Education {label:'Education', id: string, degree: string, university: string, graduationDate: string, score: string, url: string})\n",
    "Samples:\n",
    "$samples\n",
    "Question: $question\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/209280395857/locations/us-central1/models/1718472902539476992'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (c:Person)-[:HAS_SKILL]->(s1:Skill), (c)-[:HAS_SKILL]->(s2:Skill), (c)-[:HAS_SKILL]->(s3:Skill), (c)-[:HAS_SKILL]->(s4:Skill) WHERE toLower(s1.name) CONTAINS 'java' AND toLower(s2.name) CONTAINS 'python' AND toLower(s3.name) CONTAINS 'javascript' AND toLower(s4.name) CONTAINS 'security' RETURN COUNT(DISTINCT c)\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import Template\n",
    "que = 'How many are knowledgable on all of - java, python, javascript, security?'\n",
    "_prompt = Template(prompt).substitute(samples=samples, question=que)\n",
    "\n",
    "cypher = english_to_cypher_text_bison(_prompt, tuned_model_name) #for text-bison use: english_to_cypher_text_bison(_prompt, entity_extraction_tuned_model)\n",
    "if 'Answer:\\n ' in cypher:\n",
    "    cypher = cypher.split('Answer:\\n ')[1]\n",
    "cypher = cypher.replace('\\n', ' ')\n",
    "cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/209280395857/locations/us-central1/models/1718472902539476992'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.get_tuned_model(tuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vertexai.preview.language_models._PreviewTextGenerationModel at 0x1fdf34d2920>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "my_model = aip.Model(\"projects/209280395857/locations/us-central1/models/1718472902539476992\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x000001FDF3331D20> \n",
       "resource name: projects/209280395857/locations/us-central1/models/1718472902539476992"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.get_tuned_model(\"projects/209280395857/locations/us-central1/models/1718472902539476992\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vertexai.preview.language_models._PreviewTextGenerationModel at 0x1fdf7cebc70>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projects/209280395857/locations/us-central1/models/1718472902539476992']\n"
     ]
    }
   ],
   "source": [
    "tuned_model_names = model.list_tuned_model_names()\n",
    "print(tuned_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VertexAI(model_name=\"text-bison@001\", \n",
    "               tuned_model_name=\"projects/209280395857/locations/us-central1/models/1718472902539476992\",\n",
    "               max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAI(model_name='text-bison@001', client=<vertexai.language_models.TextGenerationModel object at 0x000001FDF7CF5DB0>, client_preview=<vertexai.preview.language_models._PreviewTextGenerationModel object at 0x000001FDF7CF60B0>, max_output_tokens=1024, tuned_model_name='projects/209280395857/locations/us-central1/models/1718472902539476992')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Use only Nodes and relationships mentioned in the schema\n",
    "4. Always enclose the Cypher output inside 3 backticks\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "6. Candidate node is synonymous to Person\n",
    "7. Always use aliases to refer the node in the query\n",
    "8. Cypher is NOT SQL. So, do not mix and match the syntaxes\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: How many expert java developers attend more than one universities?\n",
    "Answer: MATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_EDUCATION]->(e1:Education), (p)-[:HAS_EDUCATION]->(e2:Education) WHERE toLower(s.name) CONTAINS 'java' AND toLower(s.level) CONTAINS 'expert' AND e1.university <> e2.university RETURN COUNT(DISTINCT p)\n",
    "Question: Where do most candidates get educated?\n",
    "Answer: MATCH (p:Person)-[:HAS_EDUCATION]->(e:Education) RETURN e.university, count(e.university) as alumni ORDER BY alumni DESC LIMIT 1\n",
    "Question: How many people have worked as a Data Scientist in San Francisco?\n",
    "Answer: MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.title) CONTAINS 'data scientist' AND toLower(pos.location) CONTAINS 'san francisco' RETURN COUNT(p)\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\", \n",
    "    username=\"neo4j\", \n",
    "    password=\"Pratikps1$\"\n",
    ")\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm = llm,\n",
    "    graph=graph, verbose=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.location) CONTAINS 'london' AND pos.startDate CONTAINS '2019' RETURN p\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "r = chain(\"\"\"Which people have held a position in London with a start date in 2019\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate steps: [{'query': \"MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.location) CONTAINS 'london' AND pos.startDate CONTAINS '2019' RETURN p\"}, {'context': []}]\n",
      "Final answer: I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Intermediate steps: {r['intermediate_steps']}\")\n",
    "print(f\"Final answer: {r['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://c957c4342850b1e897.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c957c4342850b1e897.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE pos.endDate > '' RETURN COUNT(p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(p)': 286}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE pos.endDate > '' AND pos.endDate < date() + INTERVAL 5 YEAR RETURN COUNT(p)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\graphs\\neo4j_graph.py\", line 160, in query\n",
      "    data = session.run(Query(text=query, timeout=self.timeout), params)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\work\\session.py\", line 313, in run\n",
      "    self._auto_result._run(\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\work\\result.py\", line 181, in _run\n",
      "    self._attach()\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\work\\result.py\", line 301, in _attach\n",
      "    self._connection.fetch_message()\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\io\\_common.py\", line 178, in inner\n",
      "    func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\io\\_bolt.py\", line 849, in fetch_message\n",
      "    res = self._process_message(tag, fields)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\io\\_bolt5.py\", line 369, in _process_message\n",
      "    response.on_failure(summary_metadata or {})\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\neo4j\\_sync\\io\\_common.py\", line 245, in on_failure\n",
      "    raise Neo4jError.hydrate(**metadata)\n",
      "neo4j.exceptions.CypherSyntaxError: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '5': expected\r\n",
      "  \"!=\"\r\n",
      "  \"%\"\r\n",
      "  \"*\"\r\n",
      "  \"+\"\r\n",
      "  \"-\"\r\n",
      "  \"/\"\r\n",
      "  \"::\"\r\n",
      "  \"<\"\r\n",
      "  \"<=\"\r\n",
      "  \"<>\"\r\n",
      "  \"=\"\r\n",
      "  \"=~\"\r\n",
      "  \">\"\r\n",
      "  \">=\"\r\n",
      "  \"AND\"\r\n",
      "  \"CALL\"\r\n",
      "  \"CONTAINS\"\r\n",
      "  \"CREATE\"\r\n",
      "  \"DELETE\"\r\n",
      "  \"DETACH\"\r\n",
      "  \"ENDS\"\r\n",
      "  \"FOREACH\"\r\n",
      "  \"IN\"\r\n",
      "  \"IS\"\r\n",
      "  \"LOAD\"\r\n",
      "  \"MATCH\"\r\n",
      "  \"MERGE\"\r\n",
      "  \"OPTIONAL\"\r\n",
      "  \"OR\"\r\n",
      "  \"REMOVE\"\r\n",
      "  \"RETURN\"\r\n",
      "  \"SET\"\r\n",
      "  \"STARTS\"\r\n",
      "  \"UNION\"\r\n",
      "  \"UNWIND\"\r\n",
      "  \"USE\"\r\n",
      "  \"WITH\"\r\n",
      "  \"XOR\"\r\n",
      "  \"^\"\r\n",
      "  <EOF> (line 1, column 109 (offset: 108))\r\n",
      "\"MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE pos.endDate > '' AND pos.endDate < date() + INTERVAL 5 YEAR RETURN COUNT(p)\"\r\n",
      "                                                                                                             ^}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\gradio\\route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\gradio\\blocks.py\", line 1591, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\gradio\\blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Public\\anc\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\Public\\anc\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Public\\anc\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\gradio\\utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Local\\Temp\\ipykernel_69640\\1166842766.py\", line 12, in chat_response\n",
      "    response = agent_chain.run(input_text)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chains\\base.py\", line 538, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chains\\base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chains\\base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chains\\base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chains\\graph_qa\\cypher.py\", line 267, in _call\n",
      "    context = self.graph.query(generated_cypher)[: self.top_k]\n",
      "  File \"C:\\Users\\Pratik's Predator\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\graphs\\neo4j_graph.py\", line 166, in query\n",
      "    raise ValueError(f\"Generated Cypher Statement is not valid\\n{e}\")\n",
      "ValueError: Generated Cypher Statement is not valid\n",
      "{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '5': expected\r\n",
      "  \"!=\"\r\n",
      "  \"%\"\r\n",
      "  \"*\"\r\n",
      "  \"+\"\r\n",
      "  \"-\"\r\n",
      "  \"/\"\r\n",
      "  \"::\"\r\n",
      "  \"<\"\r\n",
      "  \"<=\"\r\n",
      "  \"<>\"\r\n",
      "  \"=\"\r\n",
      "  \"=~\"\r\n",
      "  \">\"\r\n",
      "  \">=\"\r\n",
      "  \"AND\"\r\n",
      "  \"CALL\"\r\n",
      "  \"CONTAINS\"\r\n",
      "  \"CREATE\"\r\n",
      "  \"DELETE\"\r\n",
      "  \"DETACH\"\r\n",
      "  \"ENDS\"\r\n",
      "  \"FOREACH\"\r\n",
      "  \"IN\"\r\n",
      "  \"IS\"\r\n",
      "  \"LOAD\"\r\n",
      "  \"MATCH\"\r\n",
      "  \"MERGE\"\r\n",
      "  \"OPTIONAL\"\r\n",
      "  \"OR\"\r\n",
      "  \"REMOVE\"\r\n",
      "  \"RETURN\"\r\n",
      "  \"SET\"\r\n",
      "  \"STARTS\"\r\n",
      "  \"UNION\"\r\n",
      "  \"UNWIND\"\r\n",
      "  \"USE\"\r\n",
      "  \"WITH\"\r\n",
      "  \"XOR\"\r\n",
      "  \"^\"\r\n",
      "  <EOF> (line 1, column 109 (offset: 108))\r\n",
      "\"MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE pos.endDate > '' AND pos.endDate < date() + INTERVAL 5 YEAR RETURN COUNT(p)\"\r\n",
      "                                                                                                             ^}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE pos.startDate <> '' AND pos.endDate <> '' RETURN COUNT(p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(p)': 286}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_EDUCATION]->(e1:Education), (p)-[:HAS_EDUCATION]->(e2:Education) WHERE toLower(s.name) CONTAINS 'java' AND e1.university <> e2.university RETURN COUNT(DISTINCT p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(DISTINCT p)': 19}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (pos:Position) WHERE toLower(pos.location) CONTAINS 'new york city' AND pos.startDate CONTAINS '2020' RETURN pos\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_POSITION]->(pos:Position)-[:AT_COMPANY]->(c:Company) WHERE toLower(s.name) CONTAINS 'java' AND toLower(c.name) CONTAINS 'amazon' RETURN p\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "llm = VertexAI(model_name='code-bison',\n",
    "            max_output_tokens=2048,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=0.40)\n",
    "agent_chain = chain\n",
    "def chat_response(input_text):\n",
    "    response = agent_chain.run(input_text)\n",
    "    return response\n",
    "\n",
    "interface = gr.Interface(fn = chat_response, inputs = \"text\", outputs = \"text\", \n",
    "                         description = \"Talent Finder Chatbot\")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
